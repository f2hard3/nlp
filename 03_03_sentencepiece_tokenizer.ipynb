{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_E = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'I was born in Korea and graduated from a University in USA.',\n",
    "]\n",
    "\n",
    "sentences_K = [\n",
    "    \"코로나가 심하다\",\n",
    "    \"코비드-19가 심하다\",\n",
    "    '아버지가방에들어가신다',\n",
    "    '아버지가 방에 들어가신다',\n",
    "    '너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'i': 2,\n",
       " 'love': 3,\n",
       " 'my': 4,\n",
       " 'dog': 5,\n",
       " 'in': 6,\n",
       " 'cat': 7,\n",
       " 'you': 8,\n",
       " 'was': 9,\n",
       " 'born': 10,\n",
       " 'korea': 11,\n",
       " 'and': 12,\n",
       " 'graduated': 13,\n",
       " 'from': 14,\n",
       " 'a': 15,\n",
       " 'university': 16,\n",
       " 'usa': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences_E)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " '심하다': 2,\n",
       " '코로나가': 3,\n",
       " '코비드': 4,\n",
       " '19가': 5,\n",
       " '아버지가방에들어가신다': 6,\n",
       " '아버지가': 7,\n",
       " '방에': 8,\n",
       " '들어가신다': 9,\n",
       " '너무너무너무는': 10,\n",
       " '나카무라세이코가': 11,\n",
       " '불러': 12,\n",
       " '크게': 13,\n",
       " '히트한': 14,\n",
       " '노래입니다': 15}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(sentences_K)\n",
    "vocabulary_keras_korean = tokenizer.word_index\n",
    "vocabulary_keras_korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코로나', '가', '심하다']\n",
      "['코', '비드', '-', '19', '가', '심하다']\n",
      "['아버지', '가방', '에', '들어가신다']\n",
      "['아버지', '가', '방', '에', '들어가신다']\n",
      "['너무', '너무', '너', '무', '는', '나카무라', '세이', '코', '가', '불러', '크게', '히트', '한', '노래', '입니다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "temp_X = []\n",
    "for sent in sentences_K:\n",
    "    temp_X.append(okt.morphs(sent))\n",
    "    print(okt.morphs(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " '가': 2,\n",
       " '심하다': 3,\n",
       " '코': 4,\n",
       " '아버지': 5,\n",
       " '에': 6,\n",
       " '들어가신다': 7,\n",
       " '너무': 8,\n",
       " '코로나': 9,\n",
       " '비드': 10,\n",
       " '-': 11,\n",
       " '19': 12,\n",
       " '가방': 13,\n",
       " '방': 14,\n",
       " '너': 15,\n",
       " '무': 16,\n",
       " '는': 17,\n",
       " '나카무라': 18,\n",
       " '세이': 19,\n",
       " '불러': 20,\n",
       " '크게': 21,\n",
       " '히트': 22,\n",
       " '한': 23,\n",
       " '노래': 24,\n",
       " '입니다': 25}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(temp_X)\n",
    "vocabulary_okt_keras = tokenizer.word_index\n",
    "vocabulary_okt_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<OOV>': 1,\n",
       "  '심하다': 2,\n",
       "  '코로나가': 3,\n",
       "  '코비드': 4,\n",
       "  '19가': 5,\n",
       "  '아버지가방에들어가신다': 6,\n",
       "  '아버지가': 7,\n",
       "  '방에': 8,\n",
       "  '들어가신다': 9,\n",
       "  '너무너무너무는': 10,\n",
       "  '나카무라세이코가': 11,\n",
       "  '불러': 12,\n",
       "  '크게': 13,\n",
       "  '히트한': 14,\n",
       "  '노래입니다': 15},\n",
       " {'<OOV>': 1,\n",
       "  '가': 2,\n",
       "  '심하다': 3,\n",
       "  '코': 4,\n",
       "  '아버지': 5,\n",
       "  '에': 6,\n",
       "  '들어가신다': 7,\n",
       "  '너무': 8,\n",
       "  '코로나': 9,\n",
       "  '비드': 10,\n",
       "  '-': 11,\n",
       "  '19': 12,\n",
       "  '가방': 13,\n",
       "  '방': 14,\n",
       "  '너': 15,\n",
       "  '무': 16,\n",
       "  '는': 17,\n",
       "  '나카무라': 18,\n",
       "  '세이': 19,\n",
       "  '불러': 20,\n",
       "  '크게': 21,\n",
       "  '히트': 22,\n",
       "  '한': 23,\n",
       "  '노래': 24,\n",
       "  '입니다': 25})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_keras_korean, vocabulary_okt_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('너무', 'Adverb'),\n",
       " ('너무', 'Adverb'),\n",
       " ('너', 'Modifier'),\n",
       " ('무', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('나카무라', 'Noun'),\n",
       " ('세이', 'Noun'),\n",
       " ('코', 'Noun'),\n",
       " ('가', 'Josa'),\n",
       " ('불러', 'Verb'),\n",
       " ('크게', 'Noun'),\n",
       " ('히트', 'Noun'),\n",
       " ('한', 'Josa'),\n",
       " ('노래', 'Noun'),\n",
       " ('입니다', 'Adjective')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.pos('너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "\n",
    "DATA_TRAIN_PATH = tf.keras.utils.get_file(\n",
    "    'ratings_train.txt', \n",
    "    'https://github.com/ironmanciti/NLP_lecture/raw/master/data/naver_movie/ratings_train.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(DATA_TRAIN_PATH, sep='\\t', quoting=3)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>6222902</td>\n",
       "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>8549745</td>\n",
       "      <td>평점이 너무 낮아서...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>9311800</td>\n",
       "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>2376369</td>\n",
       "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>9619869</td>\n",
       "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149995 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           document  label\n",
       "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
       "...          ...                                                ...    ...\n",
       "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
       "149996   8549745                                      평점이 너무 낮아서...      1\n",
       "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
       "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
       "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
       "\n",
       "[149995 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/nsmc.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_data['document'].values:\n",
    "        try:\n",
    "            f.write(line + '\\n')\n",
    "        except:\n",
    "            print('write error --->', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149996\n",
      "아 더빙.. 진짜 짜증나네요 목소리\n"
     ]
    }
   ],
   "source": [
    "with open('data/nsmc.txt', 'r', encoding='utf-8') as f:\n",
    "    nsmc_txt = f.read().split('\\n')\n",
    "print(len(nsmc_txt))\n",
    "print(nsmc_txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--input=data/nsmc.txt --model_prefix=nsmc --vocab_size=30000'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/nsmc.txt'\n",
    "prefix = 'nsmc'\n",
    "vocab_size = 30_000\n",
    "cmd = f'--input={input_file} --model_prefix={prefix} --vocab_size={vocab_size}'\n",
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/nsmc.txt --model_prefix=nsmc --vocab_size=30000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/nsmc.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: data/nsmc.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 149995 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=5435665\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=1712\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 149995 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=1911422\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 309184 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 149995\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 357580\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 357580 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=157925 obj=15.4419 num_tokens=842890 num_tokens/piece=5.33728\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=146136 obj=14.3715 num_tokens=848377 num_tokens/piece=5.80539\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=109527 obj=14.4748 num_tokens=885948 num_tokens/piece=8.08885\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=109303 obj=14.4164 num_tokens=886551 num_tokens/piece=8.11095\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=81974 obj=14.6544 num_tokens=931144 num_tokens/piece=11.359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=81965 obj=14.5922 num_tokens=931232 num_tokens/piece=11.3613\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=61473 obj=14.8562 num_tokens=974374 num_tokens/piece=15.8504\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=61473 obj=14.7933 num_tokens=974371 num_tokens/piece=15.8504\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=46104 obj=15.0862 num_tokens=1019597 num_tokens/piece=22.1152\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=46104 obj=15.0203 num_tokens=1019602 num_tokens/piece=22.1153\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=34578 obj=15.3433 num_tokens=1066442 num_tokens/piece=30.8416\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34578 obj=15.2759 num_tokens=1066422 num_tokens/piece=30.8411\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=33000 obj=15.3276 num_tokens=1073759 num_tokens/piece=32.5382\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=33000 obj=15.3173 num_tokens=1073762 num_tokens/piece=32.5382\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: nsmc.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: nsmc.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{prefix}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙.. 진짜 짜증나네요 목소리\n",
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "[53, 751, 5, 25, 15853, 1405] \n",
      "\n",
      "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "['▁흠', '...', '포스터보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "[1239, 6, 12536, 18315, 396, 47, 17886, 395, 1134, 6404, 1063, 423] \n",
      "\n",
      "너무재밓었다그래서보는것을추천한다\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는것', '을', '추천', '한다']\n",
      "[18, 611, 21195, 640, 2752, 11171, 14, 2315, 298] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in train_data['document'].values[:3]:\n",
    "    print(t)\n",
    "    print(sp.encode_as_pieces(t))\n",
    "    print(sp.encode_as_ids(t), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코로나가 심하다\n",
      "['▁코', '로', '나', '가', '▁심하다']\n",
      "[1482, 29, 33, 13, 5371]\n",
      "\n",
      "코비드-19가 심하다\n",
      "['▁코', '비', '드', '-', '19', '가', '▁심하다']\n",
      "[1482, 334, 266, 287, 3859, 13, 5371]\n",
      "\n",
      "아버지가방에들어가신다\n",
      "['▁아버지가', '방', '에', '들어가', '신', '다']\n",
      "[6158, 627, 16, 13026, 272, 23]\n",
      "\n",
      "아버지가 방에 들어가신다\n",
      "['▁아버지가', '▁방', '에', '▁들어가', '신', '다']\n",
      "[6158, 1673, 16, 3872, 272, 23]\n",
      "\n",
      "너무너무너무는 나카무라세이코가 불러 크게 히트한 노래입니다\n",
      "['▁너무너무너무', '는', '▁나카', '무라', '세', '이', '코가', '▁불러', '▁크게', '▁히트', '한', '▁노래', '입니다']\n",
      "[14213, 12, 17034, 10019, 247, 10, 12900, 3403, 1856, 12030, 30, 763, 222]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in sentences_K:\n",
    "    pieces = sp.encode_as_pieces(line)\n",
    "    ids = sp.encode_as_ids(line)\n",
    "    print(line)\n",
    "    print(pieces)\n",
    "    print(ids)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
